{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "# Worksheet 2.1:  Exploring Two Dimensional Data -  Lite Code\n",
    "This worksheet covers concepts covered in Module 2 - Exploratory Data Analysis in Two Dimensions.  It should take no more than 20-30 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "There are many ways to accomplish the tasks that you are presented with, however you will find that by using the techniques covered in class, the exercises should be relatively simple. \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (http://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Matplotlib (http://matplotlib.org/api/pyplot_api.html)\n",
    "* User Agents (https://github.com/selwin/python-user-agents)\n",
    "* Apache Log Parser (https://github.com/amandasaurus/apache-log-parser)\n",
    "\n",
    "### Data Location:\n",
    "The data location is set by default to the `data/` folder in the repo root directory.  If you have it elsewhere, you will have to modify the `DATA_HOME` variable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import apache_log_parser\n",
    "from user_agents import parse\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Reading various forms of JSON Data\n",
    "In the `/data/` folder, you will find a series of `.json` files called `dataN.json`, numbered 1-4.  Each file contains the following data:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>birthday</th>\n",
    "        <th>first_name</th>\n",
    "        <th>last_name</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>5\\/3\\/67</td>\n",
    "        <td>Robert</td>\n",
    "        <td>Hernandez</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>8\\/4\\/84</td>\n",
    "        <td>Steve</td>\n",
    "        <td>Smith</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>9\\/13\\/91</td>\n",
    "        <td>Anne</td>\n",
    "        <td>Raps</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>4\\/15\\/75</td>\n",
    "        <td>Alice</td>\n",
    "        <td>Muller</td>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n",
    "Using the `.read_json()` function and the various configuration options, read all these files into a dataframe.  The documentation is available here: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:24:11.758038Z",
     "start_time": "2025-07-11T15:24:11.744196Z"
    }
   },
   "source": [
    "DATA_HOME = '../data/'\n",
    "df1 = pd.read_json(DATA_HOME + 'data1.json')\n",
    "df1"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  first_name  last_name   birthday\n",
       "0     Robert  Hernandez   5\\/3\\/67\n",
       "1      Steve      Smith   8\\/4\\/84\n",
       "2       Anne       Raps  9\\/13\\/91\n",
       "3      Alice     Muller  4\\/15\\/75"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>birthday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robert</td>\n",
       "      <td>Hernandez</td>\n",
       "      <td>5\\/3\\/67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steve</td>\n",
       "      <td>Smith</td>\n",
       "      <td>8\\/4\\/84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anne</td>\n",
       "      <td>Raps</td>\n",
       "      <td>9\\/13\\/91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Muller</td>\n",
       "      <td>4\\/15\\/75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the documentation for the orient parameter\n",
    "df2 = pd.read_json(DATA_HOME + 'data2.json', orient=??)\n",
    "df2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df3 = pd.read_json(DATA_HOME + 'data3.json', orient=??)\n",
    "df3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df4 = pd.read_json(DATA_HOME + 'data4.json', orient=??)\n",
    "df4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: \n",
    "In the data file, there is a webserver file called `hackers-access.httpd`.  For this exercise, you will use this file to answer the following questions:\n",
    "1.  Which browsers are the top 10 most used browsers in this data?\n",
    "2.  Which are the top 10 most used operating systems?\n",
    "\n",
    "In order to accomplish this task, do the following:\n",
    "1.  Write a function which takes a User Agent string as an argument and returns the relevant data.  HINT:  You might want to use python's `user_agents` module, the documentation for which is available here: (https://pypi.python.org/pypi/user-agents)\n",
    "2.  Next, apply this function to the column which contains the user agent string.\n",
    "3.  Store this series as a new column in the dataframe\n",
    "4.  Count the occurances of each value in the new columns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "#Read in the log file\n",
    "line_parser = apache_log_parser.make_parser(\"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\"\")\n",
    "\n",
    "server_log = open(DATA_HOME + \"hackers-access.httpd\", \"r\")\n",
    "parsed_server_data = []\n",
    "for line in server_log:\n",
    "    data = {}\n",
    "    data = line_parser(line)\n",
    "    parsed_server_data.append( data )\n",
    "\n",
    "server_df = pd.DataFrame( parsed_server_data  )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "server_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write the functions\n",
    "def get_os(x: str):\n",
    "    # Use the user agent parser to get the OS family.\n",
    "    # First parse the user agent using the user agent parse\n",
    "    user_agent = # Parse the user agent\n",
    "\n",
    "    return # Get the OS family\n",
    "\n",
    "def get_browser(x: str):\n",
    "    user_agent = # Parse the user agent\n",
    "    return # Get the browser family"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Apply the functions to the dataframe\n",
    "server_df['os'] = server_df['request_header_user_agent'] # Apply the get os function\n",
    "server_df['browser'] = server_df['request_header_user_agent'] # Apply the get browser function"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Get the top 10 values for both the OS and browsers\n",
    "server_df['os']. # Your code here..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "server_df['browser']. # Your code here..",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 3:\n",
    "Using the `dailybots.csv` film, read the file into a DataFrame and perform the following operations:\n",
    "1.  Filter the DataFrame to include bots from the Government/Politics Industry.\n",
    "2.  Calculate the ratio of hosts to orgs and add this as a column to the DataFrame and output the result\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bots_df = pd.read_csv(DATA_HOME + 'dailybots.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filter the dataframe\n",
    "gov_df = bots_df[bots_df['industry'] == # Your code here.. ]\n",
    "\n",
    "# Verify the data.\n",
    "gov_df.sample(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Add the ratio\n",
    "gov_df['ratio'] = # Your code here...\n",
    "\n",
    "# Verify the data.\n",
    "gov_df.sample(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exercise 4:\n",
    "\n",
    "Read a more ```evil``` JSON ```eve_small.json```, where each line contains a nested JSON object. Derive one DataFrame, where all levels for the ```stats``` key are expanded to a top level column of that DataFrame. Easiest is to natively open the file in Python, loop over each line, use [json.loads](https://docs.python.org/3.5/library/json.html) from the json library, and then [json_normalize](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html) to expand the nested structure to top-level columns, append to a simple Python list and finally call [pd.concat](http://pandas.pydata.org/pandas-docs/version/0.20/generated/pandas.concat.html) on the list to get one complete DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:24:15.764886Z",
     "start_time": "2025-07-11T15:24:15.761873Z"
    }
   },
   "source": [
    "def nested_json_to_df(filename: str) -> pd.DataFrame:\n",
    "    with open(filename, 'r') as file:\n",
    "        data = []\n",
    "        for line in file:\n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            else:\n",
    "                # Parse the JSON data, use the json.loads() function\n",
    "                d = # Your code here...\n",
    "\n",
    "                if 'stats' in d.keys():\n",
    "                    df_tmp = # Normalize the JSON\n",
    "\n",
    "                    # Add normalized JSON to the data array\n",
    "                    data.append(df_tmp)\n",
    "\n",
    "    return pd.concat(data, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:24:17.661141Z",
     "start_time": "2025-07-11T15:24:17.605707Z"
    }
   },
   "source": [
    "evil_data = nested_json_to_df(DATA_HOME + 'eve_small.json')\n",
    "evil_data.sample(5)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "In this exercise, you will learn how to convert a PCAP file into JSON and do some basic summarization of the data.  In the `data` directory, you will find a file called `http.pcap`.  Our first step is to convert this to JSON.  To do this we have installed a python module called `pcapview` (docs available here: https://pydigger.com/pypi/pcapview) which can convert the pcap file to JSON.  \n",
    "\n",
    "Once you've done that, your assignment is to answer the following questions:\n",
    "1.  What are the most frequent source IP addresses?\n",
    "2.  How many differnet source ports were accessed?\n",
    "\n",
    "To do this you will have to load this data into a DataFrame.  Using what we've learned in class, do the following:\n",
    "1.  Load the data into a DataFrame using the technique of your choice\n",
    "2.  Extract the requisite columns from the DataFrame, in this case, you want the source IP and source ports\n",
    "3.  Execute a `value_counts()` on those columns.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Load the data\n",
    "with open(DATA_HOME + 'http-pcap.json') as data_file:    \n",
    "    pcap_data = json.load(data_file)\n",
    "\n",
    "#Normalize it and load it into a DataFrame\n",
    "pcap = pd.DataFrame( # Your code here... )\n",
    "\n",
    "#View the results\n",
    "pcap.sample(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#Extract the source port and count the unique values",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#Extract the source IP and count the unique values",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
