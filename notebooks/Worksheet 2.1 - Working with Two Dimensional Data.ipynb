{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "# Worksheet 2.1:  Working with Two Dimensional Data\n",
    "This worksheet covers concepts covered in Module 2 - Exploratory Data Analysis in Two Dimensions.  It should take no more than 20-30 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "There are many ways to accomplish the tasks that you are presented with, however you will find that by using the techniques covered in class, the exercises should be relatively simple. \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (http://pandas.pydata.org/pandas-docs/stable/)\n",
    "* json (https://docs.python.org/3/library/json.html)\n",
    "* user-agents (https://pypi.python.org/pypi/user-agents)\n",
    "* apachelogs (https://pypi.org/project/apachelogs/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:13:26.214997Z",
     "start_time": "2023-08-01T14:13:25.464958Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Reading various forms of JSON Data\n",
    "In the `/data/` folder, you will find a series of `.json` files called `dataN.json`, numbered 1-4.  Each file contains the following data:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>birthday</th>\n",
    "        <th>first_name</th>\n",
    "        <th>last_name</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>5\\/3\\/67</td>\n",
    "        <td>Robert</td>\n",
    "        <td>Hernandez</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>8\\/4\\/84</td>\n",
    "        <td>Steve</td>\n",
    "        <td>Smith</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>9\\/13\\/91</td>\n",
    "        <td>Anne</td>\n",
    "        <td>Raps</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>4\\/15\\/75</td>\n",
    "        <td>Alice</td>\n",
    "        <td>Muller</td>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n",
    "Using the `.read_json()` function and the various configuration options, read all these files into a dataframe.  The documentation is available here: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = '../data/'\n",
    "# DATA_HOME + 'data1.json' # this gives you the filename\n",
    "\n",
    "### YOUR CODE ###\n",
    "# df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE ###\n",
    "# df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE ###\n",
    "# df3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE ###\n",
    "# df4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: \n",
    "In the data file, there is a webserver file called `hackers-access.httpd`.  For this exercise, you will use this file to answer the following questions:\n",
    "1.  Which browsers are the top 3 most used browsers in this data?\n",
    "2.  Which are the least (3) used operating systems?\n",
    "\n",
    "In order to accomplish this task, do the following:\n",
    "1. Parse each log to pull out the following relevant information\n",
    "        - user agent, bytes, status, remote host, number of bytes sent\n",
    "2. For each log, store the raw text and each piece of information in a data frame\n",
    "3.  Write a function which takes a User Agent string as an argument and returns the name of the operating system and the browser family.  HINT:  You might want to use `user_agents` module, the documentation for which is available here: (https://pypi.python.org/pypi/user-agents)\n",
    "4.  Next, apply this function to the column which contains the user agent string.\n",
    "5.  Store this series as a new column in the dataframe\n",
    "6.  Count the occurances of each value in the new columns (top 5 is fine)\n",
    "\n",
    "First we need to grab another python library with pip \n",
    "[https://pypi.org/project/apachelogs/](https://pypi.org/project/apachelogs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apachelogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will take a look at how to parse a single log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_log = '192.161.57.88 - - [30/Oct/2015:15:59:39 +0100] \"POST /login_form HTTP/1.1\" 200 18354 \"http://niels.basjes.nl/login_form\" \"Mozilla/5.0 (Windows NT 6.1; rv:32.0) Gecko/20100101 Firefox/32.0\"'\n",
    "raw_text_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apachelogs import LogParser\n",
    "\n",
    "# Instatiate with a log format string\n",
    "line_parser = LogParser(\"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\"\")\n",
    "\n",
    "# create a dictionary to hols the result of the parse method\n",
    "log_dict = {}\n",
    "log_dict = line_parser.parse(raw_text_log)\n",
    "\n",
    "# to see the raw text from the log, use the entry method\n",
    "log_dict.entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other kinds of items can you pull out of this log with the methods in this class? Remember you can type <log_dict.> and hit tab to see your options, OR go to the documentation for the apachelogs library (listed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the user agent string needed for step 1:\n",
    "log_dict.headers_in[\"User-agent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import the logs from a file, parse each one and store the information in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: \n",
    "#Read in the log file\n",
    "server_log = open(DATA_HOME + \"hackers-access.httpd\", \"r\")\n",
    "\n",
    "#Create an empty dataframe\n",
    "parsed_server_data = <YOUR CODE>\n",
    "\n",
    "# read and parse each line and store the information in the columns of the dataframe\n",
    "for num, line in enumerate(server_log):\n",
    "    data = {}\n",
    "    data = line_parser.parse(<YOUR CODE>)\n",
    "    parsed_server_data.loc[num,'raw_log']= <YOUR CODE>\n",
    "    parsed_server_data.loc[num,'remote_host']= <YOUR CODE>\n",
    "    parsed_server_data.loc[num,'user_agent'] = <YOUR CODE>\n",
    "    parsed_server_data.loc[num,'request_time']= <YOUR CODE>\n",
    "    parsed_server_data.loc[num,'status']= <YOUR CODE>\n",
    "    parsed_server_data.loc[num,'bytes'] = <YOUR CODE>\n",
    "\n",
    "# view the first 5 rows\n",
    "parsed_server_data.<YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ua-parser user-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fix the functions to parse os and family from the user_agent column\n",
    "from user_agents import parse\n",
    "\n",
    "def get_os(x):\n",
    "    user_agent_os = parse(x)\n",
    "    return user_agent_os\n",
    "\n",
    "def get_browser(x):\n",
    "    user_agent_browser = parse(x).browser\n",
    "    return user_agent_browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4-5: Apply the functions to the dataframe\n",
    "\n",
    "parsed_server_data['os'] = \n",
    "parsed_server_data['browser'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Count the ocurrences of each unique value in the 2 new columns\n",
    "\n",
    "### YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 Answer\n",
    "\n",
    "### YOUR CODE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 Answer\n",
    "\n",
    "### YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:\n",
    "Using the `dailybots.csv` file, read the file into a DataFrame and perform the following operations:\n",
    "1.  Filter the DataFrame to include bots from the Government/Politics Industry.\n",
    "2.  Calculate the ratio of hosts to orgs and add this as a column to the DataFrame and output the result\n",
    "3.  Calculate the total number of hosts infected by each BotFam in the Government/Politics Industry.  You should use the `groupby()` function which is documented here: (http://pandas.pydata.org/pandas-docs/stable/groupby.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR CODE ###\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE ###\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE ###\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Exercise 4:\n",
    "\n",
    "Read a more ```evil``` JSON ```eve_small.json```, where each line contains a nested JSON object. Derive one DataFrame, where all levels for the ```stats``` key are expanded to a top level column of that DataFrame. Easiest is to natively open the file in Python, loop over each line, use [json.loads](https://docs.python.org/3.5/library/json.html) from the json library, and then [json_normalize](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html) to expand the nested structure to top-level columns, append to a simple Python list and finally call [pd.concat](http://pandas.pydata.org/pandas-docs/version/0.20/generated/pandas.concat.html) on the list to get one complete DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_json_to_df(fname_str):\n",
    "    ### YOUR CODE ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return result ### YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eve = nested_json_to_df(DATA_HOME + 'eve_small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE ###\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Exercise 5\n",
    "In this exercise, you will learn how to do some basic summarization of PCAP data.  In the `data` directory, you will find a pcap file that has been converted into json format called `http-pcap.json` (for more on converting PCAP to json see [https://kiminewt.github.io/pyshark/](https://kiminewt.github.io/pyshark/))\n",
    "\n",
    "Your assignment is to answer the following questions:\n",
    "1.  What are the most frequent source IP addresses?\n",
    "2.  How many differnet source ports were accessed?\n",
    "\n",
    "To do this you will have to load this data into a DataFrame.  Using what we've learned in class, do the following:\n",
    "1.  Load the data into a DataFrame using the technique of your choice\n",
    "2.  Extract the requisite columns from the DataFrame, in this case, you want the source IP and source ports\n",
    "3.  Execute a `value_counts()` on those columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "### YOUR CODE ###\n",
    "\n",
    "\n",
    "\n",
    "#Normalize it and load it into a DataFrame\n",
    "\n",
    "\n",
    "### YOUR CODE ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the source port and count the unique values\n",
    "### YOUR CODE ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the source IP and count the unique values\n",
    "\n",
    "### YOUR CODE ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
