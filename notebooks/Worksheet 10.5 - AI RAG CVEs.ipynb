{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "# AI: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "This notebook shows how to build a semantic search engine using **RAG**. \n",
    "\n",
    "The task is to build a model that will be able to take in a plain language query and find the most relevant documents to answer this query. \n",
    "\n",
    "#### Data: \n",
    "[https://www.kaggle.com/datasets/manavkhambhayata/cve-2024-database-exploits-cvss-os](https://www.kaggle.com/datasets/manavkhambhayata/cve-2024-database-exploits-cvss-os)\n",
    "\n",
    "The data comes from the National Vulnerability Database (NVD), a government-managed repository of cybersecurity vulnerabilities. It provides detailed information on security issues, including severity scores and affected systems.\n",
    "\n",
    "The dataset was extracted using the NVD API and processed with Python. It includes vulnerabilities published between January 1, 2024, and January 15, 2024, with key details such as CVE ID, description, CVSS score, attack vector, and affected operating systems.\n",
    "\n",
    "#### Libraries:\n",
    "\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. You will need to sign up for an account and generate an API key that you will enter below. [LangSmith](https://smith.langchain.com/?_gl=1*1c7os5z*_ga*MTk2ODA2OTQ4Ny4xNzUzODY4NjM2*_ga_47WX3HKKY2*czE3NTM4Njc4ODEkbzEkZzEkdDE3NTM4Njc4OTYkajQ1JGwwJGgw)\n",
    "\n",
    "- [langchain](https://www.langchain.com/langchain)\n",
    "- [getpass](https://docs.python.org/3/library/getpass.html) Prompt the user for a password without echoing.\n",
    "- `<a model server for embedding models> ` depending on which platform you use to pull your model embeddings, you will need their langchain library. e.g. openai, azure, this example uses huggingface\n",
    "- [langchain-huggingface](https://anaconda.org/conda-forge/langchain-huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T00:57:06.144447Z",
     "start_time": "2023-08-02T00:56:57.002456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import getpass\n",
    "import os\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Langchain has a variety of nice modules that help you load different formats of documents. [document loaders](https://python.langchain.com/docs/how_to/#document-loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = '../data/'\n",
    "\n",
    "filename_cves = 'nvd_vulnerabilities_with_os.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=DATA_HOME + filename_cves, \n",
    "    source_column='CVE ID',\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "    },\n",
    "     encoding=\"UTF-8\"\n",
    ")\n",
    "\n",
    "docs_raw = loader.load()\n",
    "for record in docs_raw[:2]:\n",
    "    print(record)\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings\n",
    "\n",
    "You can generate the templated code you need based on the model server you want to use. Go to the langchain site and select your preferred server from the dropdown menu.  They have many options, openai, azure, google, aws, anthropic, etc. Below I am using huggingface. [https://python.langchain.com/docs/tutorials/retrievers/#embeddings](https://python.langchain.com/docs/tutorials/retrievers/#embeddings)\n",
    "\n",
    "You have some decisions to make at this point: \n",
    "\n",
    "**Decision 1**: Select model server (or this is where you could access a downloaded model if working in an airgapped environment)\n",
    "\n",
    "**Decision 2**: Select the specific model to use to generate your embeddings. Since we are doing a semantic search engine, you will want something that is good at that task like a sentence-transformer. This is where you can end up with something that is great for your task, or something that is terrible. Once you have your pipeline, you will want to try out some different models to see how the results change. Things to consider:\n",
    "\n",
    " - size (larger might be better)\n",
    " - tokenizer (do you need multi-language capabilities?, this is where you would ensure it has seen your languages)\n",
    " - origin (use something that comes from a reputable source, be weary of things that have been fine-tuned by random people in places like huggingface. go for the ones from Microsoft, Facebook etc)\n",
    " - content for training (do you need to know this? if so, you'll want an open source model)\n",
    " - input length (models can only take in a specific number of tokens, older, smaller models have a smaller amount of tokens they can take in which affects your ability to embed large/long texts\n",
    "\n",
    "## Exercise 1: import your embeddings model\n",
    "Import your embeddings model from your chosen server (remember you likely need to authenticate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# you will need to grab your API token for the server you choose \n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\")# all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Embed a single document\n",
    "Try out your new embedding model on a few of our documents. The **embed_query** method is typically used to embed a single sentence, like we do for an incoming query, which is why it's useful here to just see what it does on one document. \n",
    "1. what does this embedding look like? Print a few items from it\n",
    "2. How long is this embedding? This is an important aspect of encoding documents.\n",
    "3. What does this 'length' refer to? Words? characters? sentences? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_doc = embeddings.embed_query(#your code)\n",
    "\n",
    "print(vector_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage\n",
    "Now that we know how to embed our documents, we will need to store these embeddings in a database. There are many options for doing this, but the most effecient way to store embeddings is in a **vector database**. These have been optimized to store and retrieve these kinds of embeddings, so when you can use them, you should. You can also use more traditional things like MongoBD or Elasticsearch with specific field types for storing a dense vector. This is useful if you need to store a lot of metadata or continue to have the option do to a keyword search in addition to the semantic search (this is common). \n",
    "\n",
    "## Exercise: Set up a vector store\n",
    "- You can make this more challenging by choosing something like MongoDB or pinecone if you want to play with these integrations.\n",
    "- or you can simply use the code below to store it in memory (this obviously would not be an option once we get past a few thousand items, but it will work for this exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(#your code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory = vector_store.add_documents(#Your Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search\n",
    "## Exercise: find the top 5 documents for a query and print them \n",
    "\n",
    "1. Use *similarity* search (uses string matching? or so they say on langchain site) to find the documents that are the most similar to our query.\n",
    "2. Then print the 5 most relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(\n",
    "   # YOUR Query here\n",
    ")\n",
    "\n",
    "# print your results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search 2\n",
    "\n",
    "## Exercise: Embed query and return similar docs\n",
    "\n",
    "Return documents based on similarity to an embedded query. \n",
    "1. Embed the query (USING THE SAME EMBEDDING MODEL AS THE DOCS). \n",
    "2. Compare our query's vector to the database vectors and get the ones with the smallest distance between the 2 (yes there's a function for this). There are multiple ways to calculate the *distance* between 2 vectors, but the most popular for this task is cosine similarity.\n",
    "3. Print the top 5 documents\n",
    "4. Try out a different similarity calculation and see if the results change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embeddings.embed_query(# your query)\n",
    "\n",
    "results = vector_store.similarity_search_by_vector(# your code)\n",
    "\n",
    "\n",
    "# print top 5 docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Answers\n",
    "this RAG architecture is usually part 1 of a chatbot (and many other products). This gets us the relevant source documents for a user's query, then we take these top X docs and give them to a generative model to generate an answer to our question BASED on these documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
